{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two Sigma Connect: Rental Listing Inquiries\n",
    "\n",
    "## Prerequisites\n",
    "Please make sure the following Python distributions and packages were installed.\n",
    "\n",
    "* [Anaconda](https://anaconda.org)\n",
    "* [XGBoost](https://github.com/dmlc/xgboost)\n",
    "* [LightGBM](https://github.com/Microsoft/LightGBM)\n",
    "* [Bayesian Optimization](https://github.com/fmfn/BayesianOptimization)\n",
    "* [seaborn](https://seaborn.pydata.org)\n",
    "\n",
    "\n",
    "You'll also need to create three subfolders in your working path: \n",
    "\n",
    "* input\n",
    "* output\n",
    "* python\n",
    "\n",
    "\n",
    "Then download the data files into \"input\" folder and put this notebook in \"python\" folder.\n",
    "\n",
    "The data files can be donwload from \n",
    "https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries/data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import lightgbm as lgb\n",
    "from bayes_opt import BayesianOptimization\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import preprocessing, pipeline, metrics, model_selection\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.preprocessing import Imputer\n",
    "%matplotlib inline \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get started\n",
    "\n",
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_json('../input/train.json')\n",
    "test_data = pd.read_json('../input/test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_size = train_data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create target variables\n",
    "\n",
    "We need to convert the raw target variable into numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data['target'] = train_data['interest_level'].apply(lambda x: 0 if x=='low' else 1 if x=='medium' else 2)\n",
    "train_data['low'] = train_data['interest_level'].apply(lambda x: 1 if x=='low' else 0)\n",
    "train_data['medium'] = train_data['interest_level'].apply(lambda x: 1 if x=='medium' else 0)\n",
    "train_data['high'] = train_data['interest_level'].apply(lambda x: 1 if x=='high' else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge training and testing data\n",
    "So we don't have to perform transformations twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_data=pd.concat([train_data,test_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_vars = ['bathrooms','bedrooms','latitude','longitude','price']\n",
    "cat_vars = ['building_id','manager_id','display_address','street_address']\n",
    "text_vars = ['description','features']\n",
    "date_var = 'created'\n",
    "image_var = 'photos'\n",
    "id_var = 'listing_id'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date/time features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_data['created_datetime'] = pd.to_datetime(full_data['created'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "full_data['created_year']=full_data['created_datetime'].apply(lambda x:x.year) ## low variant\n",
    "full_data['created_datetime'] = pd.to_datetime(full_data['created'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "full_data['created_month']=full_data['created_datetime'].apply(lambda x:x.month)\n",
    "full_data['created_day']=full_data['created_datetime'].apply(lambda x:x.day)\n",
    "full_data['created_dayofweek']=full_data['created_datetime'].apply(lambda x:x.dayofweek)\n",
    "full_data['created_dayofyear']=full_data['created_datetime'].apply(lambda x:x.dayofyear)\n",
    "full_data['created_weekofyear']=full_data['created_datetime'].apply(lambda x:x.weekofyear)\n",
    "full_data['created_hour']=full_data['created_datetime'].apply(lambda x:x.hour)\n",
    "full_data['created_epoch']=full_data['created_datetime'].apply(lambda x:x.value//10**9)\n",
    "\n",
    "date_num_vars = ['created_month','created_dayofweek','created_dayofyear'\n",
    "                 ,'created_weekofyear','created_hour','created_epoch']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numeric features: basic engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_data['rooms'] = full_data['bedrooms'] + full_data['bathrooms'] \n",
    "full_data['num_of_photos'] = full_data['photos'].apply(lambda x:len(x))\n",
    "full_data['num_of_features'] = full_data['features'].apply(lambda x:len(x))\n",
    "full_data['len_of_desc'] = full_data['description'].apply(lambda x:len(x))\n",
    "full_data['words_of_desc'] = full_data['description'].apply(lambda x:len(re.sub('['+string.punctuation+']', '', x).split()))\n",
    "\n",
    "\n",
    "full_data['nums_of_desc'] = full_data['description']\\\n",
    "        .apply(lambda x:re.sub('['+string.punctuation+']', '', x).split())\\\n",
    "        .apply(lambda x: len([s for s in x if s.isdigit()]))\n",
    "        \n",
    "full_data['has_phone'] = full_data['description'].apply(lambda x:re.sub('['+string.punctuation+']', '', x).split())\\\n",
    "        .apply(lambda x: [s for s in x if s.isdigit()])\\\n",
    "        .apply(lambda x: len([s for s in x if len(str(s))==10]))\\\n",
    "        .apply(lambda x: 1 if x>0 else 0)\n",
    "full_data['has_email'] = full_data['description'].apply(lambda x: 1 if '@renthop.com' in x else 0)\n",
    "\n",
    "full_data['building_id_is_zero'] = full_data['building_id'].apply(lambda x:1 if x=='0' else 0)\n",
    "\n",
    "additional_num_vars = ['rooms','num_of_photos','num_of_features','len_of_desc',\n",
    "                    'words_of_desc','has_phone','has_email','building_id_is_zero']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numeric-Numeric interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_data['avg_word_len'] = full_data[['len_of_desc','words_of_desc']]\\\n",
    "                                    .apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\n",
    "    \n",
    "full_data['price_per_room'] = full_data[['price','rooms']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\n",
    "full_data['price_per_bedroom'] = full_data[['price','bedrooms']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\n",
    "full_data['price_per_bathroom'] = full_data[['price','bathrooms']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\n",
    "full_data['price_per_feature'] = full_data[['price','num_of_features']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\n",
    "full_data['price_per_photo'] = full_data[['price','num_of_photos']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\n",
    "full_data['price_per_word'] = full_data[['price','words_of_desc']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\n",
    "full_data['price_by_desc_len'] = full_data[['price','len_of_desc']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\n",
    "\n",
    "\n",
    "full_data['photos_per_room'] = full_data[['num_of_photos','rooms']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\n",
    "full_data['photos_per_bedroom'] = full_data[['num_of_photos','bedrooms']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\n",
    "full_data['photos_per_bathroom'] = full_data[['num_of_photos','bathrooms']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\n",
    "\n",
    "full_data['desc_len_per_room'] = full_data[['len_of_desc','rooms']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\n",
    "full_data['desc_len_per_bedroom'] = full_data[['len_of_desc','bedrooms']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\n",
    "full_data['desc_len_per_bathroom'] = full_data[['len_of_desc','bathrooms']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\n",
    "full_data['desc_len_per_word'] = full_data[['len_of_desc','words_of_desc']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\n",
    "full_data['desc_len_per_numeric'] = full_data[['len_of_desc','nums_of_desc']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\n",
    "\n",
    "full_data['features_per_room'] = full_data[['num_of_features','rooms']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\n",
    "full_data['features_per_bedroom'] = full_data[['num_of_features','bedrooms']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\n",
    "full_data['features_per_bathroom'] = full_data[['num_of_features','bathrooms']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\n",
    "full_data['features_per_photo'] = full_data[['num_of_features','num_of_photos']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\n",
    "full_data['features_per_word'] = full_data[['num_of_features','words_of_desc']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\n",
    "full_data['features_by_desc_len'] = full_data[['num_of_features','len_of_desc']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\n",
    "\n",
    "\n",
    "interactive_num_vars = ['avg_word_len','price_per_room','price_per_bedroom','price_per_bathroom',\n",
    "                        'price_per_feature','price_per_photo','price_per_word','price_by_desc_len',\n",
    "                        'photos_per_room','photos_per_bedroom','photos_per_bathroom',\n",
    "                        'desc_len_per_room','desc_len_per_bedroom','desc_len_per_bathroom','desc_len_per_word','desc_len_per_numeric',\n",
    "                        'features_per_room','features_per_bedroom','features_per_bathroom',\n",
    "                        'features_per_photo','features_per_word','features_by_desc_len']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numeric-categorical interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_cat_vars =[]\n",
    "price_by_manager = full_data.groupby('manager_id')['price'].agg([np.min,np.max,np.median,np.mean]).reset_index()\n",
    "price_by_manager.columns = ['manager_id','min_price_by_manager',\n",
    "                            'max_price_by_manager','median_price_by_manager','mean_price_by_manager']\n",
    "full_data = pd.merge(full_data,price_by_manager, how='left',on='manager_id')\n",
    "\n",
    "price_by_building = full_data.groupby('building_id')['price'].agg([np.min,np.max,np.median,np.mean]).reset_index()\n",
    "price_by_building.columns = ['building_id','min_price_by_building',\n",
    "                            'max_price_by_building','median_price_by_building','mean_price_by_building']\n",
    "full_data = pd.merge(full_data,price_by_building, how='left',on='building_id')\n",
    "\n",
    "\n",
    "full_data['price_percentile_by_manager']=\\\n",
    "            full_data[['price','min_price_by_manager','max_price_by_manager']]\\\n",
    "            .apply(lambda x:(x[0]-x[1])/(x[2]-x[1]) if (x[2]-x[1])!=0 else 0.5,\n",
    "                  axis=1)\n",
    "full_data['price_percentile_by_building']=\\\n",
    "            full_data[['price','min_price_by_building','max_price_by_building']]\\\n",
    "            .apply(lambda x:(x[0]-x[1])/(x[2]-x[1]) if (x[2]-x[1])!=0 else 0.5,\n",
    "                  axis=1)\n",
    "\n",
    "\n",
    "num_cat_vars.append('price_percentile_by_manager')\n",
    "num_cat_vars.append('price_percentile_by_building')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-way categorical features interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for comb in itertools.combinations(cat_vars, 2):\n",
    "    comb_var_name = comb[0] +'-'+ comb[1]\n",
    "    full_data [comb_var_name] = full_data [ comb[0]].astype(str) +'_' + full_data [ comb[1]].astype(str)\n",
    "    cat_vars.append(comb_var_name)\n",
    "\n",
    "cat_vars    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## listing ID\n",
    "\n",
    "Theoretically ID variable is not supposed to be included in training a model. However, in this competition listing_id somehow contains the information related to listing created time so it helps improve the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_listing_id = full_data['listing_id'].min()\n",
    "max_listing_id = full_data['listing_id'].max()\n",
    "full_data['listing_id_pos']=full_data['listing_id'].apply(lambda x:np.float64((x-min_listing_id+1))/(max_listing_id-min_listing_id+1))\n",
    "num_vars.append('listing_id')\n",
    "num_vars.append('listing_id_pos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text features\n",
    "\n",
    "* Here we are using CountVectorizer but you are encouraged to give TfidfVectorizer a try.\n",
    "\n",
    "* The parameter of max_features to be tuned\n",
    "\n",
    "* The outputs are sparse matrices which can be merged with numpy arrays using scipy.stats.sparse.hstack function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_data[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\n",
    "cntvec = CountVectorizer(stop_words='english', max_features=200)\n",
    "feature_sparse =cntvec.fit_transform(full_data[\"features\"]\\\n",
    "                                     .apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x])))\n",
    "\n",
    "feature_vars = ['feature_' + v for v in cntvec.vocabulary_]\n",
    "\n",
    "cntvec = CountVectorizer(stop_words='english', max_features=100)\n",
    "desc_sparse = cntvec.fit_transform(full_data[\"description\"])\n",
    "desc_vars = ['desc_' + v for v in cntvec.vocabulary_]\n",
    "\n",
    "\n",
    "cntvec = CountVectorizer(stop_words='english', max_features=10)\n",
    "st_addr_sparse = cntvec.fit_transform(full_data[\"street_address\"])\n",
    "st_addr_vars = ['desc_' + v for v in cntvec.vocabulary_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical features - label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LBL = preprocessing.LabelEncoder()\n",
    "\n",
    "LE_vars=[]\n",
    "LE_map=dict()\n",
    "for cat_var in cat_vars:\n",
    "    print (\"Label Encoding %s\" % (cat_var))\n",
    "    LE_var=cat_var+'_le'\n",
    "    full_data[LE_var]=LBL.fit_transform(full_data[cat_var])\n",
    "    LE_vars.append(LE_var)\n",
    "    LE_map[cat_var]=LBL.classes_\n",
    "    \n",
    "print (\"Label-encoded feaures: %s\" % (LE_vars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical features - one hot encoding\n",
    "\n",
    "The output is a sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OHE = preprocessing.OneHotEncoder(sparse=True)\n",
    "start=time.time()\n",
    "OHE.fit(full_data[['building_id_le', 'manager_id_le']])\n",
    "OHE_sparse=OHE.transform(full_data[['building_id_le', 'manager_id_le']])\n",
    "                                   \n",
    "print ('One-hot-encoding finished in %f seconds' % (time.time()-start))\n",
    "\n",
    "\n",
    "OHE_vars = [var[:-3] + '_' + str(level).replace(' ','_')\\\n",
    "                for var in cat_vars for level in LE_map[var] ]\n",
    "\n",
    "print (\"OHE_sparse size :\" ,OHE_sparse.shape)\n",
    "print (\"One-hot encoded catgorical feature samples : %s\" % (OHE_vars[:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical features - mean encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from itertools import product\n",
    "\n",
    "class MeanEncoder:\n",
    "    def __init__(self, categorical_features, n_splits=5, target_type='classification', prior_weight_func=None):\n",
    "        \"\"\"\n",
    "        :param categorical_features: list of str, the name of the categorical columns to encode\n",
    "\n",
    "        :param n_splits: the number of splits used in mean encoding\n",
    "\n",
    "        :param target_type: str, 'regression' or 'classification'\n",
    "\n",
    "        :param prior_weight_func:\n",
    "        a function that takes in the number of observations, and outputs prior weight\n",
    "        when a dict is passed, the default exponential decay function will be used:\n",
    "        k: the number of observations needed for the posterior to be weighted equally as the prior\n",
    "        f: larger f --> smaller slope\n",
    "        \"\"\"\n",
    "\n",
    "        self.categorical_features = categorical_features\n",
    "        self.n_splits = n_splits\n",
    "        self.learned_stats = {}\n",
    "\n",
    "        if target_type == 'classification':\n",
    "            self.target_type = target_type\n",
    "            self.target_values = []\n",
    "        else:\n",
    "            self.target_type = 'regression'\n",
    "            self.target_values = None\n",
    "\n",
    "        if isinstance(prior_weight_func, dict):\n",
    "            self.prior_weight_func = eval('lambda x: 1 / (1 + np.exp((x - k) / f))', dict(prior_weight_func, np=np))\n",
    "        elif callable(prior_weight_func):\n",
    "            self.prior_weight_func = prior_weight_func\n",
    "        else:\n",
    "            self.prior_weight_func = lambda x: 1 / (1 + np.exp((x - 2) / 1))\n",
    "\n",
    "    @staticmethod\n",
    "    def mean_encode_subroutine(X_train, y_train, X_test, variable, target, prior_weight_func):\n",
    "        X_train = X_train[[variable]].copy()\n",
    "        X_test = X_test[[variable]].copy()\n",
    "\n",
    "        if target is not None:\n",
    "            nf_name = '{}_pred_{}'.format(variable, target)\n",
    "            X_train['pred_temp'] = (y_train == target).astype(int)  # classification\n",
    "        else:\n",
    "            nf_name = '{}_pred'.format(variable)\n",
    "            X_train['pred_temp'] = y_train  # regression\n",
    "        prior = X_train['pred_temp'].mean()\n",
    "\n",
    "        col_avg_y = X_train.groupby(by=variable, axis=0)['pred_temp'].agg({'mean': 'mean', 'beta': 'size'})\n",
    "        col_avg_y['beta'] = prior_weight_func(col_avg_y['beta'])\n",
    "        col_avg_y[nf_name] = col_avg_y['beta'] * prior + (1 - col_avg_y['beta']) * col_avg_y['mean']\n",
    "        col_avg_y.drop(['beta', 'mean'], axis=1, inplace=True)\n",
    "\n",
    "        nf_train = X_train.join(col_avg_y, on=variable)[nf_name].values\n",
    "        nf_test = X_test.join(col_avg_y, on=variable).fillna(prior, inplace=False)[nf_name].values\n",
    "\n",
    "        return nf_train, nf_test, prior, col_avg_y\n",
    "\n",
    "    def fit_transform(self, X, y):\n",
    "        \"\"\"\n",
    "        :param X: pandas DataFrame, n_samples * n_features\n",
    "        :param y: pandas Series or numpy array, n_samples\n",
    "        :return X_new: the transformed pandas DataFrame containing mean-encoded categorical features\n",
    "        \"\"\"\n",
    "        X_new = X.copy()\n",
    "        if self.target_type == 'classification':\n",
    "            skf = StratifiedKFold(self.n_splits)\n",
    "        else:\n",
    "            skf = KFold(self.n_splits)\n",
    "\n",
    "        if self.target_type == 'classification':\n",
    "            self.target_values = sorted(set(y))\n",
    "            self.learned_stats = {'{}_pred_{}'.format(variable, target): [] for variable, target in\n",
    "                                  product(self.categorical_features, self.target_values)}\n",
    "            for variable, target in product(self.categorical_features, self.target_values):\n",
    "                nf_name = '{}_pred_{}'.format(variable, target)\n",
    "                X_new.loc[:, nf_name] = np.nan\n",
    "                for large_ind, small_ind in skf.split(y, y):\n",
    "                    nf_large, nf_small, prior, col_avg_y = MeanEncoder.mean_encode_subroutine(\n",
    "                        X_new.iloc[large_ind], y.iloc[large_ind], X_new.iloc[small_ind], variable, target, self.prior_weight_func)\n",
    "                    X_new.iloc[small_ind, -1] = nf_small\n",
    "                    self.learned_stats[nf_name].append((prior, col_avg_y))\n",
    "        else:\n",
    "            self.learned_stats = {'{}_pred'.format(variable): [] for variable in self.categorical_features}\n",
    "            for variable in self.categorical_features:\n",
    "                nf_name = '{}_pred'.format(variable)\n",
    "                X_new.loc[:, nf_name] = np.nan\n",
    "                for large_ind, small_ind in skf.split(y, y):\n",
    "                    nf_large, nf_small, prior, col_avg_y = MeanEncoder.mean_encode_subroutine(\n",
    "                        X_new.iloc[large_ind], y.iloc[large_ind], X_new.iloc[small_ind], variable, None, self.prior_weight_func)\n",
    "                    X_new.iloc[small_ind, -1] = nf_small\n",
    "                    self.learned_stats[nf_name].append((prior, col_avg_y))\n",
    "        return X_new\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        :param X: pandas DataFrame, n_samples * n_features\n",
    "        :return X_new: the transformed pandas DataFrame containing mean-encoded categorical features\n",
    "        \"\"\"\n",
    "        X_new = X.copy()\n",
    "\n",
    "        if self.target_type == 'classification':\n",
    "            for variable, target in product(self.categorical_features, self.target_values):\n",
    "                nf_name = '{}_pred_{}'.format(variable, target)\n",
    "                X_new[nf_name] = 0\n",
    "                for prior, col_avg_y in self.learned_stats[nf_name]:\n",
    "                    X_new[nf_name] += X_new[[variable]].join(col_avg_y, on=variable).fillna(prior, inplace=False)[\n",
    "                        nf_name]\n",
    "                X_new[nf_name] /= self.n_splits\n",
    "        else:\n",
    "            for variable in self.categorical_features:\n",
    "                nf_name = '{}_pred'.format(variable)\n",
    "                X_new[nf_name] = 0\n",
    "                for prior, col_avg_y in self.learned_stats[nf_name]:\n",
    "                    X_new[nf_name] += X_new[[variable]].join(col_avg_y, on=variable).fillna(prior, inplace=False)[\n",
    "                        nf_name]\n",
    "                X_new[nf_name] /= self.n_splits\n",
    "\n",
    "        return X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_encoder = MeanEncoder(categorical_features=['manager_id','building_id'], prior_weight_func={'k':5, 'f':1})\n",
    "mean_encoded_train = mean_encoder.fit_transform(train_data, train_data['target'])\n",
    "mean_encoded_test = mean_encoder.transform(test_data)\n",
    "\n",
    "mean_coded_vars = list(set(mean_encoded_train.columns) - set(train_data.columns))\n",
    "mean_coded_vars.append('listing_id')\n",
    "full_data = pd.merge(full_data, \n",
    "                     pd.concat([mean_encoded_train[mean_coded_vars], mean_encoded_test[mean_coded_vars]]),\n",
    "                     how='left',\n",
    "                     on='listing_id'\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing freshness and listing quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_data['disp_is_street'] = (full_data['display_address'] == full_data['street_address'])*1\n",
    "\n",
    "full_data['num_of_html_tag']=full_data.description.apply(lambda x:x.count('<'))\n",
    "full_data['num_of_#']=full_data.description.apply(lambda x:x.count('<'))\n",
    "full_data['num_of_!']=full_data.description.apply(lambda x:x.count('<'))\n",
    "full_data['num_of_$']=full_data.description.apply(lambda x:x.count('<'))\n",
    "\n",
    "temp_aggr = full_data.sort_values(['description','building_id','bedrooms',\n",
    "                                   'bathrooms','price','created_datetime','created_datetime']).\\\n",
    "                groupby(['description','building_id','bedrooms','bathrooms','price'])\n",
    "full_data['posted_times'] = temp_aggr.created_datetime.rank(method='first', na_option='top',pct=True)\n",
    "\n",
    "num_vars = num_vars + ['disp_is_street', 'num_of_html_tag','num_of_#','num_of_!','num_of_$', 'posted_times']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manager performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "manager_agg_vars = []\n",
    "aggr_num_vars = ['bathrooms',\n",
    "                 'bedrooms',\n",
    "                 'latitude',\n",
    "                 'longitude',\n",
    "                 'price',\n",
    "                 'listing_id_pos',\n",
    "                 'building_id_is_zero',\n",
    "                 'rooms',\n",
    "                 'num_of_photos',\n",
    "                 'num_of_features',\n",
    "                 'len_of_desc',\n",
    "                 'words_of_desc',\n",
    "                 'price_per_room',\n",
    "                 'num_of_html_tag',\n",
    "                 'posted_times'\n",
    "                ]\n",
    "\n",
    "mgr_aggr =full_data.groupby('manager_id')[aggr_num_vars].agg([np.size,np.mean,np.median,np.min,np.max,np.std])\n",
    "\n",
    "\n",
    "for v in aggr_num_vars:\n",
    "    manager_agg_vars.append(v+'_'+'cnt_by_mgr')\n",
    "    manager_agg_vars.append(v+'_'+'mean_by_mgr')\n",
    "    manager_agg_vars.append(v+'_'+'median_by_mgr')\n",
    "    manager_agg_vars.append(v+'_'+'max_by_mgr')\n",
    "    manager_agg_vars.append(v+'_'+'min_by_mgr')\n",
    "    manager_agg_vars.append(v+'_'+'std_by_mgr')\n",
    "\n",
    "mgr_aggr.columns = manager_agg_vars\n",
    "\n",
    "full_data = pd.merge(full_data, mgr_aggr.reset_index(), how = 'left', on='manager_id')\n",
    "\n",
    "mgr_aggr = full_data[['manager_id','building_id']].drop_duplicates().groupby('manager_id').count().reset_index()\n",
    "mgr_aggr.columns=['manager_id','bldn_cnt_by_mgr']\n",
    "full_data = pd.merge(full_data, mgr_aggr, how = 'left', on='manager_id')\n",
    "\n",
    "mgr_aggr = full_data[['manager_id','building_id']].drop_duplicates().groupby('building_id').count().reset_index()\n",
    "mgr_aggr.columns=['building_id','mgr_cnt_by_bldn']\n",
    "full_data = pd.merge(full_data, mgr_aggr, how = 'left', on='building_id')\n",
    "\n",
    "manager_agg_vars = manager_agg_vars + ['bldn_cnt_by_mgr','mgr_cnt_by_bldn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The magic feature\n",
    "\n",
    "Firstly mentioned by Grand Master Silogram\n",
    "https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries/discussion/31765\n",
    "\n",
    "Discovered and made available to public by another Grand Master KazAnova\n",
    "https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries/discussion/31870\n",
    "\n",
    "It may contain the information when the listing was actually created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_date = pd.read_csv(\"../input/listing_image_time.csv\")\n",
    "\n",
    "image_date.columns = [\"listing_id\", \"image_time_stamp\"]\n",
    "full_data = pd.merge(full_data, image_date, on=\"listing_id\", how=\"left\")\n",
    "num_vars.append('image_time_stamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_vars = num_vars + date_num_vars + interactive_num_vars\\\n",
    "            + additional_num_vars + manager_agg_vars + LE_vars + mean_coded_vars\n",
    "    \n",
    "    \n",
    "    \n",
    "train_x = full_data[full_vars][:train_size].values\n",
    "train_y = full_data['target'][:train_size].values\n",
    "\n",
    "test_x = full_data[full_vars][train_size:].values\n",
    "\n",
    "\n",
    "train_x = sparse.hstack([train_x, feature_sparse[:train_size], \n",
    "                         desc_sparse[:train_size], st_addr_sparse[:train_size]]).tocsr()\n",
    "train_y = full_data['target'][:train_size].values\n",
    "\n",
    "test_x = sparse.hstack([test_x, feature_sparse[train_size:], \n",
    "                        desc_sparse[train_size:], st_addr_sparse[train_size:]]).tocsr()\n",
    "\n",
    "\n",
    "full_vars = full_vars + feature_vars + desc_vars + st_addr_vars + OHE_vars \n",
    "\n",
    "print (\"training data size: \", train_x.shape,\"testing data size: \", test_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Tuning\n",
    "\n",
    "### Manual tuning\n",
    "\n",
    "#### max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# scores = []\n",
    "# for max_depth in [3,4,5,6]:\n",
    "\n",
    "#     params = dict()\n",
    "#     params['objective'] = 'multi:softprob'\n",
    "#     params['num_class'] = 3\n",
    "#     params['eta'] = 0.1\n",
    "#     params['max_depth'] = max_depth\n",
    "#     params['min_child_weight'] = 1\n",
    "#     params['colsample_bytree'] = 1\n",
    "#     params['subsample'] = 1\n",
    "#     params['gamma'] = 0\n",
    "#     params['seed']=1234\n",
    "\n",
    "#     cv_results = xgb.cv(params, xgb.DMatrix(train_x, label=train_y.reshape(train_x.shape[0],1)),\n",
    "#                    num_boost_round=1000000,\n",
    "#                    nfold=5,\n",
    "#            metrics={'mlogloss'},\n",
    "#            seed=1234,\n",
    "#            callbacks=[xgb.callback.early_stop(50)])\n",
    "#     best_iteration = len(cv_results)\n",
    "#     best_score = cv_results['test-mlogloss-mean'].min()\n",
    "#     print (max_depth,best_score,best_iteration)\n",
    "#     scores.append([best_score,params['eta'],params['max_depth'],params['min_child_weight'],\n",
    "#                       params['colsample_bytree'],params['subsample'],params['gamma'],best_iteration])\n",
    "    \n",
    "# scores = pd.DataFrame(scores,columns=['score','eta','max_depth','min_child_weight',\n",
    "#                                    'colsample_bytree','subsample','gamma','best_iteration'])   \n",
    "# best_max_depth = scores.sort_values(by='score',ascending=True)['max_depth'].values[0]\n",
    "# print ('best max_depth is', best_max_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### min_child_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# scores = []\n",
    "# for min_child_weight in [1, 10, 50, 100]:\n",
    "\n",
    "#     params = dict()\n",
    "#     params['objective'] = 'multi:softprob'\n",
    "#     params['num_class'] = 3\n",
    "#     params['eta'] = 0.1\n",
    "#     params['max_depth'] = best_max_depth\n",
    "#     params['min_child_weight'] = min_child_weight\n",
    "#     params['colsample_bytree'] = 1\n",
    "#     params['subsample'] = 1\n",
    "#     params['gamma'] = 0\n",
    "#     params['seed']=1234\n",
    "\n",
    "#     cv_results = xgb.cv(params, xgb.DMatrix(train_x, label=train_y.reshape(train_x.shape[0],1)),\n",
    "#                    num_boost_round=1000000,\n",
    "#                    nfold=5,\n",
    "#            metrics={'mlogloss'},\n",
    "#            seed=1234,\n",
    "#            callbacks=[xgb.callback.early_stop(50)])\n",
    "#     best_iteration = len(cv_results)\n",
    "#     best_score = cv_results['test-mlogloss-mean'].min()\n",
    "#     print (min_child_weight,best_score,best_iteration)\n",
    "#     scores.append([best_score,params['eta'],params['max_depth'],params['min_child_weight'],\n",
    "#                       params['colsample_bytree'],params['subsample'],params['gamma'],best_iteration])\n",
    "    \n",
    "# scores = pd.DataFrame(scores,columns=['score','eta','max_depth','min_child_weight',\n",
    "#                                    'colsample_bytree','subsample','gamma','best_iteration'])   \n",
    "# best_min_child_weight = scores.sort_values(by='score',ascending=True)['min_child_weight'].values[0]\n",
    "# print ('best min_child_weight is', best_min_child_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### colsample_bytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# scores = []\n",
    "# for colsample_bytree in [0.1,0.3,0.5,0.7,0.9]:\n",
    "\n",
    "#     params = dict()\n",
    "#     params['objective'] = 'multi:softprob'\n",
    "#     params['num_class'] = 3\n",
    "#     params['eta'] = 0.1\n",
    "#     params['max_depth'] = best_max_depth\n",
    "#     params['min_child_weight'] = best_min_child_weight\n",
    "#     params['colsample_bytree'] = colsample_bytree\n",
    "#     params['subsample'] = 1\n",
    "#     params['gamma'] = 0\n",
    "#     params['seed']=1234\n",
    "\n",
    "#     cv_results = xgb.cv(params, xgb.DMatrix(train_x, label=train_y.reshape(train_x.shape[0],1)),\n",
    "#                    num_boost_round=1000000,\n",
    "#                    nfold=5,\n",
    "#            metrics={'mlogloss'},\n",
    "#            seed=1234,\n",
    "#            callbacks=[xgb.callback.early_stop(50)])\n",
    "#     best_iteration = len(cv_results)\n",
    "#     best_score = cv_results['test-mlogloss-mean'].min()\n",
    "#     print (colsample_bytree,best_score,best_iteration)\n",
    "#     scores.append([best_score,params['eta'],params['max_depth'],params['min_child_weight'],\n",
    "#                       params['colsample_bytree'],params['subsample'],params['gamma'],best_iteration])\n",
    "    \n",
    "# scores = pd.DataFrame(scores,columns=['score','eta','max_depth','min_child_weight',\n",
    "#                                    'colsample_bytree','subsample','gamma','best_iteration'])   \n",
    "# best_colsample_bytree = scores.sort_values(by='score',ascending=True)['colsample_bytree'].values[0]\n",
    "# print ('best colsample_bytree is', best_colsample_bytree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# scores = []\n",
    "# for subsample in [0.1,0.3,0.5,0.7,0.9]:\n",
    "\n",
    "#     params = dict()\n",
    "#     params['objective'] = 'multi:softprob'\n",
    "#     params['num_class'] = 3\n",
    "#     params['eta'] = 0.1\n",
    "#     params['max_depth'] = best_max_depth\n",
    "#     params['min_child_weight'] = best_min_child_weight\n",
    "#     params['colsample_bytree'] = best_colsample_bytree\n",
    "#     params['subsample'] = subsample\n",
    "#     params['gamma'] = 0\n",
    "#     params['seed']=1234\n",
    "\n",
    "#     cv_results = xgb.cv(params, xgb.DMatrix(train_x, label=train_y.reshape(train_x.shape[0],1)),\n",
    "#                    num_boost_round=1000000,\n",
    "#                    nfold=5,\n",
    "#            metrics={'mlogloss'},\n",
    "#            seed=1234,\n",
    "#            callbacks=[xgb.callback.early_stop(50)])\n",
    "#     best_iteration = len(cv_results)\n",
    "#     best_score = cv_results['test-mlogloss-mean'].min()\n",
    "#     print (subsample,best_score,best_iteration)\n",
    "#     scores.append([best_score,params['eta'],params['max_depth'],params['min_child_weight'],\n",
    "#                       params['colsample_bytree'],params['subsample'],params['gamma'],best_iteration])\n",
    "    \n",
    "# scores = pd.DataFrame(scores,columns=['score','eta','max_depth','min_child_weight',\n",
    "#                                    'colsample_bytree','subsample','gamma','best_iteration'])   \n",
    "# best_subsample = scores.sort_values(by='score',ascending=True)['subsample'].values[0]\n",
    "# print ('best subsample is', best_subsample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# scores = []\n",
    "# for gamma in [0,0.5,1,1.5,2]:\n",
    "\n",
    "#     params = dict()\n",
    "#     params['objective'] = 'multi:softprob'\n",
    "#     params['num_class'] = 3\n",
    "#     params['eta'] = 0.1\n",
    "#     params['max_depth'] = best_max_depth\n",
    "#     params['min_child_weight'] = best_min_child_weight\n",
    "#     params['colsample_bytree'] = best_colsample_bytree\n",
    "#     params['subsample'] = best_subsample\n",
    "#     params['gamma'] = gamma\n",
    "#     params['seed']=1234\n",
    "\n",
    "#     cv_results = xgb.cv(params, xgb.DMatrix(train_x, label=train_y.reshape(train_x.shape[0],1)),\n",
    "#                    num_boost_round=1000000,\n",
    "#                    nfold=5,\n",
    "#            metrics={'mlogloss'},\n",
    "#            seed=1234,\n",
    "#            callbacks=[xgb.callback.early_stop(50)])\n",
    "#     best_iteration = len(cv_results)\n",
    "#     best_score = cv_results['test-mlogloss-mean'].min()\n",
    "#     print (gamma,best_score,best_iteration)\n",
    "#     scores.append([best_score,params['eta'],params['max_depth'],params['min_child_weight'],\n",
    "#                       params['colsample_bytree'],params['subsample'],params['gamma'],best_iteration])\n",
    "    \n",
    "# scores = pd.DataFrame(scores,columns=['score','eta','max_depth','min_child_weight',\n",
    "#                                    'colsample_bytree','subsample','gamma','best_iteration'])   \n",
    "# best_gamma = scores.sort_values(by='score',ascending=True)['gamma'].values[0]\n",
    "# print ('best gamma is', best_gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated tuning\n",
    "\n",
    "We will be using Bayesian optimization for automated parameter tuning.\n",
    "\n",
    "It works by constructing a posterior distribution of functions (gaussian process) that best describes the function you want to optimize. As the number of observations grows, the posterior distribution improves, and the algorithm becomes more certain of which regions in parameter space are worth exploring and which are not, as seen in the picture below.\n",
    "\n",
    "* https://github.com/fmfn/BayesianOptimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xgtrain = xgb.DMatrix(train_x, label=train_y.reshape(train_x.shape[0],1))\n",
    "\n",
    "def xgb_evaluate(min_child_weight,\n",
    "                 colsample_bytree,\n",
    "                 max_depth,\n",
    "                 subsample,\n",
    "                 gamma):\n",
    "    params = dict()\n",
    "    params['objective'] = 'multi:softprob'\n",
    "    params['num_class'] = 3\n",
    "    params['eta'] = 0.1\n",
    "    params['max_depth'] = int(max_depth )   \n",
    "    params['min_child_weight'] = int(min_child_weight)\n",
    "    params['colsample_bytree'] = colsample_bytree\n",
    "    params['subsample'] = subsample\n",
    "    params['gamma'] = gamma\n",
    "    params['verbose_eval'] = True    \n",
    "\n",
    "\n",
    "    cv_result = xgb.cv(params, xgtrain,\n",
    "                       num_boost_round=100000,\n",
    "                       nfold=5,\n",
    "                       metrics={'mlogloss'},\n",
    "                       seed=1234,\n",
    "                       callbacks=[xgb.callback.early_stop(50)])\n",
    "\n",
    "    return -cv_result['test-mlogloss-mean'].min()\n",
    "\n",
    "\n",
    "xgb_BO = BayesianOptimization(xgb_evaluate, \n",
    "                             {'max_depth': (3, 10),\n",
    "                              'min_child_weight': (0, 100),\n",
    "                              'colsample_bytree': (0.1, 0.7),\n",
    "                              'subsample': (0.7, 1),\n",
    "                              'gamma': (0, 2)\n",
    "                             }\n",
    "                            )\n",
    "\n",
    "xgb_BO.maximize(init_points=5, n_iter=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show tuning results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_BO_scores = pd.DataFrame(xgb_BO.res['all']['params'])\n",
    "xgb_BO_scores['score'] = pd.DataFrame(xgb_BO.res['all']['values'])\n",
    "xgb_BO_scores = xgb_BO_scores.sort_values(by='score',ascending=False)\n",
    "xgb_BO_scores.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot scores vs parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(xgb_BO_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model with smaller learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = xgb_BO_scores.iloc[0].to_dict()\n",
    "xgb_params['objective'] = 'multi:softprob'\n",
    "xgb_params['num_class'] = 3\n",
    "xgb_params['eta'] = 0.01 # Smaller \n",
    "\n",
    "xgb_params['max_depth'] = int(xgb_params['max_depth'])   \n",
    "xgb_params['min_child_weight'] = int(xgb_params['min_child_weight'])    \n",
    "xgb_params['subsample'] = xgb_params['subsample']     \n",
    "xgb_params['colsample_bytree'] = xgb_params['colsample_bytree']\n",
    "xgb_params['gamma'] = xgb_params['gamma']\n",
    "xgb_params['seed']=1234\n",
    "\n",
    "cv_results = xgb.cv(xgb_params, \n",
    "                    xgb.DMatrix(train_x, label=train_y.reshape(train_x.shape[0],1)),\n",
    "                    num_boost_round=1000000, \n",
    "                    nfold=5,\n",
    "                    metrics={'mlogloss'},\n",
    "                    seed=1234,\n",
    "                    callbacks=[xgb.callback.early_stop(50)],\n",
    "                    verbose_eval=50\n",
    "                   )\n",
    "\n",
    "best_xgb_score = cv_results['test-mlogloss-mean'].min()\n",
    "best_xgb_iteration = len(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "clf = xgb.XGBClassifier(learning_rate = 0.01\n",
    "                        , n_estimators =best_xgb_iteration\n",
    "                        , max_depth = xgb_params['max_depth']\n",
    "                        , min_child_weight = xgb_params['min_child_weight']\n",
    "                        , subsample = xgb_params['subsample']\n",
    "                        , colsample_bytree = xgb_params['colsample_bytree']\n",
    "                        , gamma = xgb_params['gamma']\n",
    "                        , seed = 1234\n",
    "                        , nthread = -1\n",
    "                       )\n",
    "\n",
    "clf.fit(train_x, train_y)\n",
    "\n",
    "print (\"Training finished in %d seconds.\" % (time.time()-start))\n",
    "\n",
    "preds = clf.predict_proba(test_x)\n",
    "sub_df = pd.DataFrame(preds,columns = [\"low\", \"medium\", \"high\"])\n",
    "sub_df[\"listing_id\"] = test_data.listing_id.values\n",
    "sub_df.to_csv(\"../output/sub_xgb_tuned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM\n",
    "\n",
    "#### Manual tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# scores = []\n",
    "# for max_bin in [100, 255, 400,600,800,1000]:\n",
    "\n",
    "#     params = dict()\n",
    "#     params['objective'] = 'multiclass'\n",
    "#     params['num_class'] = 3\n",
    "#     params['learning_rate'] = 0.1\n",
    "#     params['max_bin'] = max_bin  \n",
    "\n",
    "#     cv_results = lgb.cv(params,\n",
    "#                     lgb.Dataset(train_x, train_y, max_bin=max_bin),\n",
    "#                     num_boost_round=1000000,\n",
    "#                     nfold=5,\n",
    "#                     early_stopping_rounds=100,\n",
    "#                     metrics='multi_logloss',\n",
    "#                     shuffle=False,\n",
    "#                     verbose_eval=100\n",
    "#                    )\n",
    "#     cv_results = pd.DataFrame(cv_results)\n",
    "#     best_iteration = len(cv_results)\n",
    "#     best_score = cv_results['multi_logloss-mean'].min()\n",
    "#     print (max_bin,best_iteration, best_score)\n",
    "#     scores.append([max_bin,best_iteration, best_score])\n",
    "    \n",
    "# scores = pd.DataFrame(scores, columns =['max_bin','iteration','score'])\n",
    "# best_max_bin = scores.sort_values(by='score',ascending=True)['max_bin'].values[0]\n",
    "# print ('best max_bin is', best_max_bin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# scores = []\n",
    "# for num_leaves in [3, 10, 30,100,300,1000]:\n",
    "\n",
    "#     params = dict()\n",
    "#     params['objective'] = 'multiclass'\n",
    "#     params['num_class'] = 3\n",
    "#     params['learning_rate'] = 0.1\n",
    "#     params['max_bin'] = 800  \n",
    "#     params['num_leaves'] = num_leaves\n",
    "\n",
    "#     cv_results = lgb.cv(params,\n",
    "#                     lgb.Dataset(train_x, train_y, max_bin=best_max_bin),\n",
    "#                     num_boost_round=1000000,\n",
    "#                     nfold=5,\n",
    "#                     early_stopping_rounds=100,\n",
    "#                     metrics='multi_logloss',\n",
    "#                     shuffle=False,\n",
    "#                     verbose_eval=100\n",
    "#                    )\n",
    "#     cv_results = pd.DataFrame(cv_results)\n",
    "#     best_iteration = len(cv_results)\n",
    "#     best_score = cv_results['multi_logloss-mean'].min()\n",
    "#     print (num_leaves,best_score,best_iteration)\n",
    "#     scores.append([num_leaves,best_iteration, best_score])\n",
    "\n",
    "# scores = pd.DataFrame(scores, columns =['num_leaves','iteration','score'])\n",
    "# best_num_leaves = scores.sort_values(by='score',ascending=True)['num_leaves'].values[0]\n",
    "# print ('best num_leaves is', best_num_leaves)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# scores = []\n",
    "# for feature_fraction in [0.2, 0.4, 0.6, 0.8]:\n",
    "\n",
    "#     params = dict()\n",
    "#     params['objective'] = 'multiclass'\n",
    "#     params['num_class'] = 3\n",
    "#     params['learning_rate'] = 0.1\n",
    "#     params['max_bin'] = best_max_bin  \n",
    "#     params['num_leaves'] = best_num_leaves\n",
    "#     params['feature_fraction'] = feature_fraction\n",
    "\n",
    "#     cv_results = lgb.cv(params,\n",
    "#                     lgb.Dataset(train_x, train_y, max_bin=best_max_bin),\n",
    "#                     num_boost_round=1000000,\n",
    "#                     nfold=5,\n",
    "#                     early_stopping_rounds=100,\n",
    "#                     metrics='multi_logloss',\n",
    "#                     shuffle=False,\n",
    "#                     verbose_eval=100\n",
    "#                    )\n",
    "#     cv_results = pd.DataFrame(cv_results)\n",
    "#     best_iteration = len(cv_results)\n",
    "#     best_score = cv_results['multi_logloss-mean'].min()\n",
    "#     print (feature_fraction,best_iteration, best_score)\n",
    "#     scores.append([feature_fraction,best_iteration, best_score])\n",
    "\n",
    "# scores = pd.DataFrame(scores, columns =['feature_fraction','iteration','score'])\n",
    "# best_feature_fraction = scores.sort_values(by='score',ascending=True)['feature_fraction'].values[0]\n",
    "# print ('best feature_fraction is', best_feature_fraction)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# scores = []\n",
    "# for bagging_fraction in [0.3, 0.5, 0.7, 0.9]:\n",
    "\n",
    "#     params = dict()\n",
    "#     params['objective'] = 'multiclass'\n",
    "#     params['num_class'] = 3\n",
    "#     params['learning_rate'] = 0.1\n",
    "#     params['max_bin'] = best_max_bin\n",
    "#     params['num_leaves'] = best_num_leaves\n",
    "#     params['feature_fraction'] = best_feature_fraction\n",
    "#     params['bagging_fraction'] = bagging_fraction\n",
    "#     params['bagging_freq'] = 1\n",
    "\n",
    "#     cv_results = lgb.cv(params,\n",
    "#                     lgb.Dataset(train_x, train_y, max_bin=best_max_bin),\n",
    "#                     num_boost_round=1000000,\n",
    "#                     nfold=5,\n",
    "#                     early_stopping_rounds=100,\n",
    "#                     metrics='multi_logloss',\n",
    "#                     shuffle=False,\n",
    "#                     verbose_eval=100\n",
    "#                    )\n",
    "#     cv_results = pd.DataFrame(cv_results)\n",
    "#     best_iteration = len(cv_results)\n",
    "#     best_score = cv_results['multi_logloss-mean'].min()\n",
    "#     print (bagging_fraction,best_iteration,best_score)\n",
    "#     scores.append([bagging_fraction,best_iteration,best_score])\n",
    "\n",
    "# scores = pd.DataFrame(scores, columns =['bagging_fraction','iteration','score'])\n",
    "# best_bagging_fraction = scores.sort_values(by='score',ascending=True)['bagging_fraction'].values[0]\n",
    "# print ('best bagging_fraction is', best_bagging_fraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# scores = []\n",
    "# for bagging_fraction in [1]:\n",
    "\n",
    "#     params = dict()\n",
    "#     params['objective'] = 'multiclass'\n",
    "#     params['num_class'] = 3\n",
    "#     params['learning_rate'] = 0.1\n",
    "#     params['max_bin'] = best_max_bin\n",
    "#     params['num_leaves'] = best_num_leaves\n",
    "#     params['feature_fraction'] = best_feature_fraction\n",
    "#     params['bagging_fraction'] = bagging_fraction\n",
    "#     params['bagging_freq'] = 1\n",
    "\n",
    "#     cv_results = lgb.cv(params,\n",
    "#                     lgb.Dataset(train_x, train_y, max_bin=best_max_bin),\n",
    "#                     num_boost_round=1000000,\n",
    "#                     nfold=5,\n",
    "#                     early_stopping_rounds=100,\n",
    "#                     metrics='multi_logloss',\n",
    "#                     shuffle=False,\n",
    "#                     verbose_eval=100\n",
    "#                    )\n",
    "#     cv_results = pd.DataFrame(cv_results)\n",
    "#     best_iteration = len(cv_results)\n",
    "#     best_score = cv_results['multi_logloss-mean'].min()\n",
    "#     print (bagging_fraction,best_iteration,best_score)\n",
    "#     scores.append([bagging_fraction,best_iteration,best_score])\n",
    "\n",
    "# scores = pd.DataFrame(scores, columns =['bagging_fraction','iteration','score'])\n",
    "# best_bagging_fraction = scores.sort_values(by='score',ascending=True)['bagging_fraction'].values[0]\n",
    "# print ('best bagging_fraction is', best_bagging_fraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# scores = []\n",
    "# for bagging_freq in [1, 3, 5]:\n",
    "\n",
    "#     params = dict()\n",
    "#     params['objective'] = 'multiclass'\n",
    "#     params['num_class'] = 3\n",
    "#     params['learning_rate'] = 0.1\n",
    "#     params['max_bin'] = best_max_bin\n",
    "#     params['num_leaves'] = best_num_leaves\n",
    "#     params['feature_fraction'] = best_feature_fraction\n",
    "#     params['bagging_fraction'] = best_bagging_fraction\n",
    "#     params['bagging_freq'] = bagging_freq\n",
    "\n",
    "#     cv_results = lgb.cv(params,\n",
    "#                     lgb.Dataset(train_x, train_y, max_bin=best_max_bin),\n",
    "#                     num_boost_round=1000000,\n",
    "#                     nfold=5,\n",
    "#                     early_stopping_rounds=100,\n",
    "#                     metrics='multi_logloss',\n",
    "#                     shuffle=False,\n",
    "#                     verbose_eval=100\n",
    "#                    )\n",
    "#     cv_results = pd.DataFrame(cv_results)\n",
    "#     best_iteration = len(cv_results)\n",
    "#     best_score = cv_results['multi_logloss-mean'].min()\n",
    "#     print (bagging_freq,best_iteration,best_score)\n",
    "#     scores.append([bagging_freq,best_iteration,best_score])\n",
    "\n",
    "# scores = pd.DataFrame(scores, columns =['bagging_freq','iteration','score'])\n",
    "# best_bagging_freq = scores.sort_values(by='score',ascending=True)['bagging_freq'].values[0]\n",
    "# print ('best bagging_freq is', best_bagging_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# scores = []\n",
    "# for min_gain_to_split in [0, 0.1, 0.5, 1.0, 1.5]:\n",
    "\n",
    "#     params = dict()\n",
    "#     params['objective'] = 'multiclass'\n",
    "#     params['num_class'] = 3\n",
    "#     params['learning_rate'] = 0.1\n",
    "#     params['max_bin'] = best_max_bin\n",
    "#     params['num_leaves'] = best_num_leaves\n",
    "#     params['feature_fraction'] = best_feature_fraction\n",
    "#     params['bagging_fraction'] = best_bagging_fraction\n",
    "#     params['min_gain_to_split'] = min_gain_to_split\n",
    "\n",
    "#     cv_results = lgb.cv(params,\n",
    "#                     lgb.Dataset(train_x, train_y, max_bin=best_max_bin),\n",
    "#                     num_boost_round=1000000,\n",
    "#                     nfold=5,\n",
    "#                     early_stopping_rounds=100,\n",
    "#                     metrics='multi_logloss',\n",
    "#                     shuffle=False,\n",
    "#                     verbose_eval=100\n",
    "#                    )\n",
    "#     cv_results = pd.DataFrame(cv_results)\n",
    "#     best_iteration = len(cv_results)\n",
    "#     best_score = cv_results['multi_logloss-mean'].min()\n",
    "#     print (min_gain_to_split,best_iteration,best_score)\n",
    "#     scores.append([min_gain_to_split,best_iteration,best_score])\n",
    "\n",
    "# scores = pd.DataFrame(scores, columns =['min_gain_to_split','iteration','score'])\n",
    "# best_min_gain_to_split = scores.sort_values(by='score',ascending=True)['min_gain_to_split'].values[0]\n",
    "# print ('best min_gain_to_split is', best_min_gain_to_split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# scores = []\n",
    "# for min_sum_hessian_in_leaf in [0,0.001,1, 3, 10,30,100]:\n",
    "\n",
    "#     params = dict()\n",
    "#     params['objective'] = 'multiclass'\n",
    "#     params['num_class'] = 3\n",
    "#     params['learning_rate'] = 0.1\n",
    "#     params['max_bin'] = best_max_bin\n",
    "#     params['num_leaves'] = best_num_leaves\n",
    "#     params['feature_fraction'] = best_feature_fraction\n",
    "#     params['bagging_fraction'] = best_bagging_fraction\n",
    "#     params['min_gain_to_split'] = best_min_gain_to_split\n",
    "#     params['min_sum_hessian_in_leaf'] = min_sum_hessian_in_leaf\n",
    "\n",
    "#     cv_results = lgb.cv(params,\n",
    "#                     lgb.Dataset(train_x, train_y, max_bin=best_max_bin),\n",
    "#                     num_boost_round=1000000,\n",
    "#                     nfold=5,\n",
    "#                     early_stopping_rounds=100,\n",
    "#                     metrics='multi_logloss',\n",
    "#                     shuffle=False,\n",
    "#                     verbose_eval=100\n",
    "#                    )\n",
    "#     cv_results = pd.DataFrame(cv_results)\n",
    "#     best_iteration = len(cv_results)\n",
    "#     best_score = cv_results['multi_logloss-mean'].min()\n",
    "#     print (min_sum_hessian_in_leaf,best_iteration,best_score)\n",
    "#     scores.append([min_sum_hessian_in_leaf,best_iteration,best_score])\n",
    "\n",
    "# scores = pd.DataFrame(scores, columns =['min_sum_hessian_in_leaf','iteration','score'])\n",
    "# best_min_sum_hessian_in_leaf = scores.sort_values(by='score',ascending=True)['min_sum_hessian_in_leaf'].values[0]\n",
    "# print ('best min_sum_hessian_in_leaf is', best_min_sum_hessian_in_leaf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# scores = []\n",
    "# for min_sum_hessian_in_leaf in [0,0.001,1, 3, 10,30,100]:\n",
    "\n",
    "#     params = dict()\n",
    "#     params['objective'] = 'multiclass'\n",
    "#     params['num_class'] = 3\n",
    "#     params['learning_rate'] = 0.1\n",
    "#     params['max_bin'] = best_max_bin\n",
    "#     params['num_leaves'] = best_num_leaves\n",
    "#     params['feature_fraction'] = best_feature_fraction\n",
    "#     params['bagging_fraction'] = best_bagging_fraction\n",
    "#     params['min_gain_to_split'] = best_min_gain_to_split\n",
    "#     params['min_sum_hessian_in_leaf'] = min_sum_hessian_in_leaf\n",
    "\n",
    "#     cv_results = lgb.cv(params,\n",
    "#                     lgb.Dataset(train_x, train_y, max_bin=best_max_bin),\n",
    "#                     num_boost_round=1000000,\n",
    "#                     nfold=5,\n",
    "#                     early_stopping_rounds=100,\n",
    "#                     metrics='multi_logloss',\n",
    "#                     shuffle=False,\n",
    "#                     verbose_eval=100\n",
    "#                    )\n",
    "#     cv_results = pd.DataFrame(cv_results)\n",
    "#     best_iteration = len(cv_results)\n",
    "#     best_score = cv_results['multi_logloss-mean'].min()\n",
    "#     print (min_sum_hessian_in_leaf,best_iteration,best_score)\n",
    "#     scores.append([min_sum_hessian_in_leaf,best_iteration,best_score])\n",
    "\n",
    "# scores = pd.DataFrame(scores, columns =['min_sum_hessian_in_leaf','iteration','score'])\n",
    "# best_min_sum_hessian_in_leaf = scores.sort_values(by='score',ascending=True)['min_sum_hessian_in_leaf'].values[0]\n",
    "# print ('best min_sum_hessian_in_leaf is', best_min_sum_hessian_in_leaf)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# scores = []\n",
    "# for lambda_l2 in [0,0.01,0.1, 1, 10]:\n",
    "\n",
    "#     params = dict()\n",
    "#     params['objective'] = 'multiclass'\n",
    "#     params['num_class'] = 3\n",
    "#     params['learning_rate'] = 0.1\n",
    "#     params['max_bin'] = best_max_bin  \n",
    "#     params['num_leaves'] = best_num_leaves\n",
    "#     params['feature_fraction'] = best_feature_fraction\n",
    "#     params['bagging_fraction'] = best_bagging_fraction\n",
    "#     params['min_gain_to_split'] = best_min_gain_to_split\n",
    "#     params['min_sum_hessian_in_leaf'] = best_min_sum_hessian_in_leaf\n",
    "#     params['lambda_l1'] = best_lambda_l1\n",
    "#     params['lambda_l2'] = lambda_l2\n",
    "\n",
    "#     cv_results = lgb.cv(params,\n",
    "#                     lgb.Dataset(train_x, train_y, max_bin=best_max_bin),\n",
    "#                     num_boost_round=1000000,\n",
    "#                     nfold=5,\n",
    "#                     early_stopping_rounds=100,\n",
    "#                     metrics='multi_logloss',\n",
    "#                     shuffle=False,\n",
    "#                     verbose_eval=100\n",
    "#                    )\n",
    "#     cv_results = pd.DataFrame(cv_results)\n",
    "#     best_iteration = len(cv_results)\n",
    "#     best_score = cv_results['multi_logloss-mean'].min()\n",
    "#     print (lambda_l2,best_iteration,best_score)\n",
    "#     scores.append([lambda_l2,best_iteration,best_score])\n",
    "\n",
    "# scores = pd.DataFrame(scores, columns =['lambda_l2','iteration','score'])\n",
    "# best_lambda_l2 = scores.sort_values(by='score',ascending=True)['lambda_l2'].values[0]\n",
    "# print ('best best_lambda_l2 is', best_lambda_l2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def lgb_evaluate(max_bin,\n",
    "                 num_leaves,\n",
    "                 min_sum_hessian_in_leaf,\n",
    "                 min_gain_to_split,\n",
    "                 feature_fraction,\n",
    "                 bagging_fraction,\n",
    "                 bagging_freq,\n",
    "                 lambda_l1,\n",
    "                 lambda_l2\n",
    "                 ):\n",
    "    params = dict()\n",
    "    params['objective'] = 'multiclass'\n",
    "    params['num_class'] = 3\n",
    "    params['learning_rate'] = 0.1\n",
    "    params['max_bin'] = int(max_bin)\n",
    "    params['num_leaves'] = int(num_leaves)    \n",
    "    params['min_sum_hessian_in_leaf'] = int(min_sum_hessian_in_leaf)\n",
    "    params['min_gain_to_split'] = min_gain_to_split    \n",
    "    params['feature_fraction'] = feature_fraction\n",
    "    params['bagging_fraction'] = bagging_fraction\n",
    "    params['bagging_freq'] = int(bagging_freq)\n",
    "\n",
    "\n",
    "    cv_results = lgb.cv(params,\n",
    "                    lgb.Dataset(train_x, train_y, max_bin=int(max_bin)),\n",
    "                    num_boost_round=1000000,\n",
    "                    nfold=5,\n",
    "                    early_stopping_rounds=100,\n",
    "                    metrics='multi_logloss',\n",
    "                    stratified=False,\n",
    "                    shuffle=True,\n",
    "                    verbose_eval=False\n",
    "                   )\n",
    "\n",
    "    return -pd.DataFrame(cv_results)['multi_logloss-mean'].min()\n",
    "\n",
    "\n",
    "lgb_BO = BayesianOptimization(lgb_evaluate, \n",
    "                             {'max_bin': (850, 900),\n",
    "                              'num_leaves': (10, 20),\n",
    "                              'min_sum_hessian_in_leaf': (4, 8),\n",
    "                              'min_gain_to_split': (0,1),\n",
    "                              'feature_fraction': (0.35, 0.45),\n",
    "                              'bagging_fraction': (0.8,1),\n",
    "                              'bagging_freq': (1,1),\n",
    "                              'lambda_l1': (0,0.5),\n",
    "                              'lambda_l2': (0,10)\n",
    "                             }\n",
    "                            )\n",
    "\n",
    "lgb_BO.maximize(init_points=5, n_iter=40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show LightGBM tuning results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_BO_scores = pd.DataFrame(lgb_BO.res['all']['params'])\n",
    "lgb_BO_scores['score'] = pd.DataFrame(lgb_BO.res['all']['values'])\n",
    "lgb_BO_scores = lgb_BO_scores.sort_values(by='score',ascending=False)\n",
    "lgb_BO_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model with smaller learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "params = lgb_BO_scores.iloc[0].to_dict()\n",
    "lgb_params = dict()\n",
    "lgb_params['objective'] = 'multiclass'\n",
    "lgb_params['num_class'] = 3\n",
    "lgb_params['learning_rate'] = 0.01 # Smaller learning rate\n",
    "\n",
    "lgb_params['max_bin'] = int(params['max_bin'])   \n",
    "lgb_params['num_leaves'] = int(params['num_leaves'])    \n",
    "lgb_params['min_sum_hessian_in_leaf'] = int(params['min_sum_hessian_in_leaf'])\n",
    "lgb_params['min_gain_to_split'] = params['min_gain_to_split']     \n",
    "lgb_params['feature_fraction'] = params['feature_fraction']\n",
    "lgb_params['bagging_fraction'] = params['bagging_fraction']\n",
    "lgb_params['bagging_freq'] = int(params['bagging_freq'])\n",
    "\n",
    "\n",
    "cv_results = lgb.cv(lgb_params,\n",
    "                lgb.Dataset(train_x, train_y, max_bin=lgb_params['max_bin']),\n",
    "                num_boost_round=1000000,\n",
    "                nfold=5,\n",
    "                early_stopping_rounds=200, # Bigger stopping rounds\n",
    "                metrics='multi_logloss',\n",
    "                shuffle=True, stratified=False,\n",
    "                verbose_eval=100\n",
    "               )\n",
    "\n",
    "cv_results = pd.DataFrame(cv_results)\n",
    "best_lgb_iteration = len(cv_results)\n",
    "best_lgb_score = cv_results['multi_logloss-mean'].min()\n",
    "\n",
    "print (best_lgb_iteration, best_lgb_score)\n",
    "\n",
    "\n",
    "# [100]\tcv_agg's multi_logloss: 0.746109 + 0.0065575\n",
    "# [200]\tcv_agg's multi_logloss: 0.639026 + 0.0120335\n",
    "# [300]\tcv_agg's multi_logloss: 0.596377 + 0.0147712\n",
    "# [400]\tcv_agg's multi_logloss: 0.574696 + 0.0158541\n",
    "# [500]\tcv_agg's multi_logloss: 0.562016 + 0.0162295\n",
    "# [600]\tcv_agg's multi_logloss: 0.553874 + 0.0161019\n",
    "# [700]\tcv_agg's multi_logloss: 0.548233 + 0.0157665\n",
    "# [800]\tcv_agg's multi_logloss: 0.544046 + 0.0156151\n",
    "# [900]\tcv_agg's multi_logloss: 0.540636 + 0.0154918\n",
    "# [1000]\tcv_agg's multi_logloss: 0.537844 + 0.0153443\n",
    "# [1100]\tcv_agg's multi_logloss: 0.535618 + 0.0151677\n",
    "# [1200]\tcv_agg's multi_logloss: 0.533848 + 0.0150771\n",
    "# [1300]\tcv_agg's multi_logloss: 0.53235 + 0.0150731\n",
    "# [1400]\tcv_agg's multi_logloss: 0.531319 + 0.0150225\n",
    "# [1500]\tcv_agg's multi_logloss: 0.530262 + 0.0149934\n",
    "# [1600]\tcv_agg's multi_logloss: 0.529346 + 0.0149552\n",
    "# [1700]\tcv_agg's multi_logloss: 0.528506 + 0.0149328\n",
    "# [1800]\tcv_agg's multi_logloss: 0.527763 + 0.0148955\n",
    "# [1900]\tcv_agg's multi_logloss: 0.527144 + 0.0148148\n",
    "# [2000]\tcv_agg's multi_logloss: 0.526505 + 0.014854\n",
    "# [2100]\tcv_agg's multi_logloss: 0.525989 + 0.0148448\n",
    "# [2200]\tcv_agg's multi_logloss: 0.525539 + 0.0148273\n",
    "# [2300]\tcv_agg's multi_logloss: 0.525226 + 0.0147968\n",
    "# [2400]\tcv_agg's multi_logloss: 0.524854 + 0.0147791\n",
    "# [2500]\tcv_agg's multi_logloss: 0.524553 + 0.0147948\n",
    "# [2600]\tcv_agg's multi_logloss: 0.524289 + 0.014782\n",
    "# [2700]\tcv_agg's multi_logloss: 0.524064 + 0.0147793\n",
    "# [2800]\tcv_agg's multi_logloss: 0.523813 + 0.0147561\n",
    "# [2900]\tcv_agg's multi_logloss: 0.52353 + 0.0147243\n",
    "# [3000]\tcv_agg's multi_logloss: 0.523274 + 0.0146968\n",
    "# [3100]\tcv_agg's multi_logloss: 0.523125 + 0.0146779\n",
    "# [3200]\tcv_agg's multi_logloss: 0.522853 + 0.0146509\n",
    "# [3300]\tcv_agg's multi_logloss: 0.522641 + 0.0146358\n",
    "# [3400]\tcv_agg's multi_logloss: 0.522566 + 0.0146042\n",
    "# [3500]\tcv_agg's multi_logloss: 0.522362 + 0.0145816\n",
    "# [3600]\tcv_agg's multi_logloss: 0.522202 + 0.0145666\n",
    "# [3700]\tcv_agg's multi_logloss: 0.522101 + 0.0145976\n",
    "# [3800]\tcv_agg's multi_logloss: 0.521914 + 0.0146079\n",
    "# [3900]\tcv_agg's multi_logloss: 0.521868 + 0.0145966\n",
    "# [4000]\tcv_agg's multi_logloss: 0.521732 + 0.0145751\n",
    "# [4100]\tcv_agg's multi_logloss: 0.521581 + 0.0145607\n",
    "# [4200]\tcv_agg's multi_logloss: 0.521465 + 0.0145596\n",
    "# [4300]\tcv_agg's multi_logloss: 0.521352 + 0.0145497\n",
    "# [4400]\tcv_agg's multi_logloss: 0.521356 + 0.0145549\n",
    "# [4500]\tcv_agg's multi_logloss: 0.521287 + 0.0145471\n",
    "# [4600]\tcv_agg's multi_logloss: 0.521228 + 0.0145317\n",
    "# [4700]\tcv_agg's multi_logloss: 0.521191 + 0.0145207\n",
    "# [4800]\tcv_agg's multi_logloss: 0.521114 + 0.0145103\n",
    "# [4900]\tcv_agg's multi_logloss: 0.521096 + 0.0144984\n",
    "# [5000]\tcv_agg's multi_logloss: 0.521048 + 0.0145049\n",
    "# [5100]\tcv_agg's multi_logloss: 0.521157 + 0.0145078\n",
    "# [5200]\tcv_agg's multi_logloss: 0.521136 + 0.0144979\n",
    "# 5021 0.521042308018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "clf = lgb.LGBMClassifier(learning_rate = 0.01\n",
    "                        , n_estimators =best_lgb_iteration\n",
    "                        , max_bin = lgb_params['max_bin']   \n",
    "                        , num_leaves = lgb_params['num_leaves']\n",
    "                        , min_child_weight = lgb_params['min_sum_hessian_in_leaf']\n",
    "                        , min_split_gain = lgb_params['min_gain_to_split'] \n",
    "                        , colsample_bytree = lgb_params['feature_fraction']\n",
    "                        , subsample = lgb_params['bagging_fraction']\n",
    "                        , subsample_freq = lgb_params['bagging_freq']\n",
    "                        , seed = 1234\n",
    "                       )\n",
    "\n",
    "print (clf)\n",
    "\n",
    "clf.fit(train_x, train_y)\n",
    "\n",
    "print (\"Training finished in %d seconds.\" % (time.time()-start))\n",
    "\n",
    "preds = clf.predict_proba(test_x)\n",
    "sub_df = pd.DataFrame(preds,columns = [\"low\", \"medium\", \"high\"])\n",
    "sub_df[\"listing_id\"] = test_data.listing_id.values\n",
    "sub_df.to_csv(\"../output/sub_lgb_tuned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model stacking\n",
    "\n",
    "1. We'll leverage the tuned parameter sets to train 5 XGBoost models and 5 LightGBM models for level 1.\n",
    "2. The outputs of level models will contain 3(classes)*10(models) = 15 features. \n",
    "3. We'll train a MLP model using these 15 features only\n",
    "4. We'll train another LightGBM using these 15 features plus original features\n",
    "5. The outputs from the two level 2 models can be combined as the final submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's define  a few stacking fucntions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def blend_lgb_model(params_list, train_x, train_y, test_x, num_class, blend_folds):\n",
    "\n",
    "    #     skf = model_selection.StratifiedKFold(n_splits=blend_folds,random_state=1234)\n",
    "    skf = model_selection.KFold(\n",
    "        n_splits=blend_folds, random_state=1234)\n",
    "    skf_ids = list(skf.split(train_x, train_y))\n",
    "\n",
    "    train_blend_x = np.zeros((train_x.shape[0], len(params_list) * num_class))\n",
    "    test_blend_x = np.zeros((test_x.shape[0], len(params_list) * num_class))\n",
    "    blend_scores = np.zeros((blend_folds, len(params_list)))\n",
    "\n",
    "    print(\"Start blending.\")\n",
    "    for j, params in enumerate(params_list):\n",
    "        print(\"Blending model\", j + 1, params)\n",
    "        test_blend_x_j = np.zeros((test_x.shape[0], num_class))\n",
    "        max_bin = params['max_bin']\n",
    "        num_boost_round = params['num_boost_round']\n",
    "        for i, (train_ids, val_ids) in enumerate(skf_ids):\n",
    "            start = time.time()\n",
    "            print(\"Model %d fold %d\" % (j + 1, i + 1))\n",
    "            train_x_fold = train_x[train_ids]\n",
    "            train_y_fold = train_y[train_ids]\n",
    "            val_x_fold = train_x[val_ids]\n",
    "            val_y_fold = train_y[val_ids]\n",
    "            # Set n_estimators to a large number for early_stopping\n",
    "            print(params)\n",
    "            model = lgb.train(params,\n",
    "                              lgb.Dataset(train_x_fold, train_y_fold,\n",
    "                                          max_bin=max_bin),\n",
    "                              num_boost_round=num_boost_round\n",
    "                              )\n",
    "            val_y_predict_fold = model.predict(val_x_fold)\n",
    "            score = metrics.log_loss(val_y_fold, val_y_predict_fold)\n",
    "            print(\"LOGLOSS: \", score)\n",
    "            blend_scores[i, j] = score\n",
    "            train_blend_x[val_ids, j * num_class:j *\n",
    "                          num_class + num_class] = val_y_predict_fold\n",
    "            test_blend_x_j = test_blend_x_j + model.predict(test_x)\n",
    "            print(time.time() - start)\n",
    "        test_blend_x[:, j * num_class:j * num_class +\n",
    "                     num_class] = test_blend_x_j / blend_folds\n",
    "        print(\"Score for model %d is %f\" %\n",
    "              (j + 1, np.mean(blend_scores[:, j])))\n",
    "    return train_blend_x, test_blend_x, blend_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def blend_xgb_model(params_list, train_x, train_y, test_x, num_class, blend_folds,missing=None):\n",
    "\n",
    "    skf = model_selection.KFold(n_splits=blend_folds,random_state=1234)\n",
    "    skf_ids = list(skf.split(train_x, train_y))\n",
    "\n",
    "\n",
    "    train_blend_x = np.zeros((train_x.shape[0], len(params_list)*num_class))\n",
    "    test_blend_x = np.zeros((test_x.shape[0], len(params_list)*num_class))\n",
    "    blend_scores = np.zeros ((blend_folds,len(params_list)))\n",
    "\n",
    "    print  (\"Start blending.\")\n",
    "    for j, params in enumerate(params_list):\n",
    "        print (\"Blending model\",j+1, params)\n",
    "        test_blend_x_j = np.zeros((test_x.shape[0], num_class))\n",
    "        for i, (train_ids, val_ids) in enumerate(skf_ids):\n",
    "            start = time.time()\n",
    "            print (\"Model %d fold %d\" %(j+1,i+1))\n",
    "            train_x_fold = train_x[train_ids]\n",
    "            train_y_fold = train_y[train_ids]\n",
    "            val_x_fold = train_x[val_ids]\n",
    "            val_y_fold = train_y[val_ids]\n",
    "            # Set n_estimators to a large number for early_stopping   \n",
    "            print (params, params['num_boost_round'], missing)\n",
    "            model = xgb.train(params,\n",
    "                                xgb.DMatrix(train_x_fold, \n",
    "                                            label=train_y_fold.reshape(train_y_fold.shape[0],1), \n",
    "                                            missing=missing),\n",
    "                                num_boost_round=params['num_boost_round']\n",
    "                            )\n",
    "            val_y_predict_fold = model.predict(xgb.DMatrix(val_x_fold,missing=missing),\n",
    "                                              )\n",
    "            \n",
    "            score = metrics.log_loss(val_y_fold,val_y_predict_fold)\n",
    "            print (\"LOGLOSS: \", score)\n",
    "            blend_scores[i,j]=score\n",
    "            train_blend_x[val_ids, j*num_class:j*num_class+num_class] = val_y_predict_fold\n",
    "            test_blend_x_j = test_blend_x_j + model.predict(xgb.DMatrix(test_x,missing=missing))\n",
    "            print (time.time()-start)\n",
    "        test_blend_x[:,j*num_class:j*num_class+num_class] = test_blend_x_j/blend_folds\n",
    "        print (\"Score for model %d is %f\" % (j+1,np.mean(blend_scores[:,j])))\n",
    "    return train_blend_x, test_blend_x, blend_scores    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Another version: using early stopping\n",
    "\n",
    "# def blend_xgb_model(params_list, train_x, train_y, test_x, num_class, blend_folds,missing=None):\n",
    "\n",
    "#     skf = model_selection.KFold(n_splits=blend_folds,random_state=1234, shuffle=True)\n",
    "#     skf_ids = list(skf.split(train_x, train_y))\n",
    "\n",
    "\n",
    "#     train_blend_x = np.zeros((train_x.shape[0], len(params_list)*num_class))\n",
    "#     test_blend_x = np.zeros((test_x.shape[0], len(params_list)*num_class))\n",
    "#     blend_scores = np.zeros ((blend_folds,len(params_list)))\n",
    "\n",
    "#     print  (\"Start blending.\")\n",
    "#     for j, params in enumerate(params_list):\n",
    "#         print (\"Blending model\",j+1, params)\n",
    "#         test_blend_x_j = np.zeros((test_x.shape[0], num_class))\n",
    "#         for i, (train_ids, val_ids) in enumerate(skf_ids):\n",
    "#             start = time.time()\n",
    "#             print (\"Model %d fold %d\" %(j+1,i+1))\n",
    "#             train_x_fold = train_x[train_ids]\n",
    "#             train_y_fold = train_y[train_ids]\n",
    "#             val_x_fold = train_x[val_ids]\n",
    "#             val_y_fold = train_y[val_ids]\n",
    "#             # Set n_estimators to a large number for early_stopping   \n",
    "#             print (params, params['num_boost_round'], missing)\n",
    "#             model = xgb.train(params,\n",
    "#                               xgb.DMatrix(train_x_fold, \n",
    "#                                           label=train_y_fold.reshape(train_y_fold.shape[0],1), \n",
    "#                                           missing=missing),\n",
    "#                               evals = [(xgb.DMatrix(val_x_fold, \n",
    "#                                                     val_y_fold, \n",
    "#                                                     missing=missing), 'valid')],\n",
    "#                               num_boost_round=10000,\n",
    "#                               verbose_eval=100, \n",
    "#                               early_stopping_rounds=100\n",
    "#                              )    \n",
    "#             val_y_predict_fold = model.predict(xgb.DMatrix(val_x_fold,missing=missing), \n",
    "#                                                     ntree_limit = model.best_iteration\n",
    "#                                               )\n",
    "            \n",
    "#             score = metrics.log_loss(val_y_fold,val_y_predict_fold)\n",
    "#             print (\"LOGLOSS: \", score)\n",
    "#             blend_scores[i,j]=score\n",
    "#             train_blend_x[val_ids, j*num_class:j*num_class+num_class] = val_y_predict_fold\n",
    "#             test_blend_x_j = test_blend_x_j + model.predict(xgb.DMatrix(test_x,missing=missing))\n",
    "#             print (time.time()-start)\n",
    "#         test_blend_x[:,j*num_class:j*num_class+num_class] = test_blend_x_j/blend_folds\n",
    "#         print (\"Score for model %d is %f\" % (j+1,np.mean(blend_scores[:,j])))\n",
    "#     return train_blend_x, test_blend_x, blend_scores    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression,RidgeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "def search_model(train_x, train_y, est, param_grid, n_jobs, cv, refit=False):\n",
    "##Grid Search for the best model\n",
    "    model = model_selection.GridSearchCV(estimator  = est,\n",
    "                                     param_grid = param_grid,\n",
    "                                     scoring    = 'log_loss',\n",
    "                                     verbose    = 10,\n",
    "                                     n_jobs  = n_jobs,\n",
    "                                     iid        = True,\n",
    "                                     refit    = refit,\n",
    "                                     cv      = cv)\n",
    "    # Fit Grid Search Model\n",
    "    model.fit(train_x, train_y)\n",
    "    print(\"Best score: %0.3f\" % model.best_score_)\n",
    "    print(\"Best parameters set:\", model.best_params_)\n",
    "    print(\"Scores:\", model.grid_scores_)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Level 1 models\n",
    "\n",
    "#### LightGBM\n",
    "\n",
    "* Train 5 models based on top 5 sets of parameters tuned by Bayesian Optimization\n",
    "* Save the outputs they can be used later for model ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params_list = []\n",
    "\n",
    "for p in lgb_BO_scores.head(5).iterrows(): #Top 5 sets of params\n",
    "    params = dict()\n",
    "    params['objective'] = 'multiclass'\n",
    "    params['num_class'] = 3\n",
    "    params['learning_rate'] = 0.01\n",
    "    params['max_bin'] = int(p[1].to_dict()['max_bin'])\n",
    "    params['num_leaves'] = int(p[1].to_dict()['num_leaves'])   \n",
    "    params['min_sum_hessian_in_leaf'] = int(p[1].to_dict()['min_sum_hessian_in_leaf'])   \n",
    "    params['min_gain_to_split'] = p[1].to_dict()['min_gain_to_split']    \n",
    "    params['feature_fraction'] = p[1].to_dict()['feature_fraction']\n",
    "    params['bagging_fraction'] = p[1].to_dict()['bagging_fraction']\n",
    "    params['bagging_freq'] = int(p[1].to_dict()['bagging_freq'])\n",
    "    params['num_boost_round'] = best_lgb_iteration\n",
    "    print (params)\n",
    "    lgb_params_list.append(params) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lgb_params_list = [{'min_gain_to_split': 0.77275621547590578, 'bagging_fraction': 0.93193799681414446, 'learning_rate': 0.01, 'bagging_freq': 1, 'min_sum_hessian_in_leaf': 4, 'feature_fraction': 0.42634174534518871, 'max_bin': 870, 'num_leaves': 19, 'objective': 'multiclass', 'num_boost_round': 10, 'num_class': 3},\n",
    "# {'min_gain_to_split': 0.78091813895986428, 'bagging_fraction': 0.98937474204062581, 'learning_rate': 0.01, 'bagging_freq': 1, 'min_sum_hessian_in_leaf': 7, 'feature_fraction': 0.43295950599989869, 'max_bin': 890, 'num_leaves': 19, 'objective': 'multiclass', 'num_boost_round': 10, 'num_class': 3},\n",
    "# {'min_gain_to_split': 0.87710698158147304, 'bagging_fraction': 0.94705819160185833, 'learning_rate': 0.01, 'bagging_freq': 1, 'min_sum_hessian_in_leaf': 7, 'feature_fraction': 0.36031255532444145, 'max_bin': 899, 'num_leaves': 10, 'objective': 'multiclass', 'num_boost_round': 10, 'num_class': 3},\n",
    "# {'min_gain_to_split': 0.78341159495834189, 'bagging_fraction': 0.96529206675756984, 'learning_rate': 0.01, 'bagging_freq': 1, 'min_sum_hessian_in_leaf': 7, 'feature_fraction': 0.38724806130539013, 'max_bin': 893, 'num_leaves': 19, 'objective': 'multiclass', 'num_boost_round': 10, 'num_class': 3},\n",
    "# {'min_gain_to_split': 0.94107377908690526, 'bagging_fraction': 0.85962423257426801, 'learning_rate': 0.01, 'bagging_freq': 1, 'min_sum_hessian_in_leaf': 7, 'feature_fraction': 0.43604871956773872, 'max_bin': 899, 'num_leaves': 19, 'objective': 'multiclass', 'num_boost_round': 10, 'num_class': 3}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_blend_x_lgb_01, test_blend_x_lgb_01, blend_scores_lgb_01 = blend_lgb_model(lgb_params_list, \n",
    "                                                        train_x, \n",
    "                                                        train_y, \n",
    "                                                        test_x, \n",
    "                                                        num_class=3, \n",
    "                                                        blend_folds=5)\n",
    "\n",
    "\n",
    "np.savetxt('../input/train_blend_x_lgb_01.csv',train_blend_x_lgb_01, delimiter=',')\n",
    "np.savetxt('../input/test_blend_x_lgb_01.csv',test_blend_x_lgb_01, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll create a sumbision using the output from model 1, which was generated from out-of-sample predictions instead of a single model. Compare to the submission generated by a single model, which one is better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = test_blend_x_lgb_01[:,0:3]\n",
    "sub_df = pd.DataFrame(preds,columns = [\"low\", \"medium\", \"high\"])\n",
    "sub_df[\"listing_id\"] = test_data.listing_id.values\n",
    "sub_df.to_csv(\"../output/sub_lgb_tuned_oos.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Level 1 LightGBM features -> Level 2 MLP\n",
    "\n",
    "Here we are training a MLP model with features from Level 1 LightGBM models. How what one scores as compared to previous ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "              \"hidden_layer_sizes\":[50,100,200,(200,100),(200,100,50)]\n",
    "              }\n",
    "model = search_model(train_blend_x_lgb_01\n",
    "                                         , train_y\n",
    "                                         , MLPClassifier()\n",
    "                                         , param_grid\n",
    "                                         , n_jobs=1\n",
    "                                         , cv=4\n",
    "                                         , refit=True)   \n",
    "\n",
    "print (\"best subsample:\", model.best_params_)\n",
    "\n",
    "\n",
    "preds = model.predict_proba(test_blend_x_lgb_01)\n",
    "sub_df = pd.DataFrame(preds,columns = [\"low\", \"medium\", \"high\"])\n",
    "sub_df[\"listing_id\"] = test_data.listing_id.values\n",
    "sub_df.to_csv(\"../output/sub_lgb_stacked_mlp.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Level 1 LightGBM features + original features -> Level 2 LightGBM\n",
    "\n",
    "Here we are training a LightGBM model with features from Level 1 LightGBM models along with original features. How what one scores as compared to previous ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = lgb_BO_scores.iloc[0].to_dict()\n",
    "lgb_params = dict()\n",
    "lgb_params['objective'] = 'multiclass'\n",
    "lgb_params['num_class'] = 3\n",
    "lgb_params['learning_rate'] = 0.01 # Smaller learning rate\n",
    "\n",
    "lgb_params['max_bin'] = int(params['max_bin'])   \n",
    "lgb_params['num_leaves'] = int(params['num_leaves'])    \n",
    "lgb_params['min_sum_hessian_in_leaf'] = int(params['min_sum_hessian_in_leaf'])\n",
    "lgb_params['min_gain_to_split'] = params['min_gain_to_split']     \n",
    "lgb_params['feature_fraction'] = params['feature_fraction']\n",
    "lgb_params['bagging_fraction'] = params['bagging_fraction']\n",
    "lgb_params['bagging_freq'] = 1\n",
    "\n",
    "\n",
    "cv_results = lgb.cv(lgb_params,\n",
    "                lgb.Dataset(sparse.hstack([train_x,train_blend_x_lgb_01]).tocsr(),\n",
    "                            train_y, \n",
    "                            max_bin=lgb_params['max_bin']\n",
    "                           ),\n",
    "                num_boost_round=1000000,\n",
    "                nfold=5,\n",
    "                early_stopping_rounds=200, # Bigger stopping rounds\n",
    "                metrics='multi_logloss',\n",
    "                shuffle=False,\n",
    "                verbose_eval=100\n",
    "               )\n",
    "\n",
    "cv_results = pd.DataFrame(cv_results)\n",
    "best_lgb_iteration = len(cv_results)\n",
    "best_lgb_score = cv_results['multi_logloss-mean'].min()\n",
    "\n",
    "print (best_lgb_iteration, best_lgb_score)\n",
    "\n",
    "start = time.time()\n",
    "clf = lgb.LGBMClassifier(learning_rate = 0.01\n",
    "                        , n_estimators =best_lgb_iteration\n",
    "                        , max_bin = lgb_params['max_bin']   \n",
    "                        , num_leaves = lgb_params['num_leaves']\n",
    "                        , min_child_weight = lgb_params['min_sum_hessian_in_leaf']\n",
    "                        , min_split_gain = lgb_params['min_gain_to_split'] \n",
    "                        , colsample_bytree = lgb_params['feature_fraction']\n",
    "                        , subsample = lgb_params['bagging_fraction']\n",
    "                        , subsample_freq = lgb_params['bagging_freq']\n",
    "                        , seed = 1234\n",
    "                       )\n",
    "\n",
    "print (clf)\n",
    "\n",
    "clf.fit(sparse.hstack([train_x,train_blend_x_lgb_01]).tocsr(), train_y)\n",
    "\n",
    "print (\"Training finished in %d seconds.\" % (time.time()-start))\n",
    "\n",
    "preds = clf.predict_proba(sparse.hstack([test_x, test_blend_x_lgb_01]).tocsr())\n",
    "sub_df = pd.DataFrame(preds,columns = [\"low\", \"medium\", \"high\"])\n",
    "sub_df[\"listing_id\"] = test_data.listing_id.values\n",
    "sub_df.to_csv(\"../output/sub_lgb_stacked_lgb.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result seems promising. But what if we added XGBoost models to the ensemble? Would it help?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Level 1 models\n",
    "\n",
    "#### XGBoost\n",
    "\n",
    "* Train 5 XGBoost models based on top 5 sets of parameters tuned by Bayesian Optimization\n",
    "* Save the outputs they can be used later for model ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "stopping_rounds = 100\n",
    "\n",
    "xgb_param_list = []\n",
    "\n",
    "for p in xgb_BO_scores[:5].iterrows():\n",
    "    start = time.time()\n",
    "    params = dict()\n",
    "    params['objective'] = 'multi:softprob'\n",
    "    params['num_boost_round'] = best_xgb_iteration\n",
    "    params['num_class'] = 3\n",
    "    params['eta'] = 0.01\n",
    "    params['max_depth'] = int(p[1].to_dict()['max_depth'])\n",
    "    params['min_child_weight'] = int(p[1].to_dict()['min_child_weight'])\n",
    "    params['subsample'] = p[1].to_dict()['subsample']\n",
    "    params['colsample_bytree'] = p[1].to_dict()['colsample_bytree']\n",
    "    params['gamma'] = p[1].to_dict()['gamma']\n",
    "    params['seed']=1234\n",
    "    xgb_param_list.append(params)\n",
    "\n",
    "print(xgb_param_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_blend_x_xgb_01, test_blend_x_xgb_01, blend_scores_xgb_01 = blend_xgb_model(xgb_param_list, \n",
    "                                                        train_x, \n",
    "                                                        train_y, \n",
    "                                                        test_x, \n",
    "                                                        num_class=3, \n",
    "                                                        blend_folds=5)  \n",
    "                                                        \n",
    "                                                        \n",
    "np.savetxt('../input/train_blend_x_xgb_01.csv',train_blend_x_xgb_01, delimiter=',')\n",
    "np.savetxt('../input/test_blend_x_xgb_01.csv',test_blend_x_xgb_01, delimiter=',')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Level 1 LightGBM features + level 1 XGBoost features -> Level 2 MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "              \"hidden_layer_sizes\":[50,100,200,(200,100),(200,100,50)]\n",
    "              }\n",
    "model = search_model(np.hstack((train_blend_x_lgb_01, train_blend_x_xgb_01))\n",
    "                                         , train_y\n",
    "                                         , MLPClassifier()\n",
    "                                         , param_grid\n",
    "                                         , n_jobs=-1\n",
    "                                         , cv=4\n",
    "                                         , refit=True)   \n",
    "\n",
    "print (\"best subsample:\", model.best_params_)\n",
    "\n",
    "\n",
    "preds_mlp = model.predict_proba(np.hstack((test_blend_x_lgb_01, test_blend_x_xgb_01)))\n",
    "sub_df_mlp = pd.DataFrame(preds_mlp,columns = [\"low\", \"medium\", \"high\"])\n",
    "sub_df_mlp[\"listing_id\"] = test_data.listing_id.values\n",
    "sub_df_mlp.to_csv(\"../output/sub_lgb_xgb_stacked_mlp.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Level 1 LightGBM features + level 1 XGBoost features  + original features -> Level 2 LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = lgb_BO_scores.iloc[0].to_dict()\n",
    "lgb_params = dict()\n",
    "lgb_params['objective'] = 'multiclass'\n",
    "lgb_params['num_class'] = 3\n",
    "lgb_params['learning_rate'] = 0.01 # Smaller learning rate\n",
    "\n",
    "lgb_params['max_bin'] = int(params['max_bin'])   \n",
    "lgb_params['num_leaves'] = int(params['num_leaves'])    \n",
    "lgb_params['min_sum_hessian_in_leaf'] = int(params['min_sum_hessian_in_leaf'])\n",
    "lgb_params['min_gain_to_split'] = params['min_gain_to_split']     \n",
    "lgb_params['feature_fraction'] = params['feature_fraction']\n",
    "lgb_params['bagging_fraction'] = params['bagging_fraction']\n",
    "lgb_params['bagging_freq'] = 1\n",
    "\n",
    "\n",
    "cv_results = lgb.cv(lgb_params,\n",
    "                lgb.Dataset(sparse.hstack([train_x,train_blend_x_lgb_01, train_blend_x_xgb_01]).tocsr(),\n",
    "                            train_y, \n",
    "                            max_bin=lgb_params['max_bin']\n",
    "                           ),\n",
    "                num_boost_round=1000000,\n",
    "                nfold=5,\n",
    "                early_stopping_rounds=200, # Bigger stopping rounds\n",
    "                metrics='multi_logloss',\n",
    "                shuffle=False,\n",
    "                verbose_eval=100\n",
    "               )\n",
    "\n",
    "cv_results = pd.DataFrame(cv_results)\n",
    "best_lgb_iteration = len(cv_results)\n",
    "best_lgb_score = cv_results['multi_logloss-mean'].min()\n",
    "\n",
    "print (best_lgb_iteration, best_lgb_score)\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "clf = lgb.LGBMClassifier(learning_rate = 0.01\n",
    "                        , n_estimators =best_lgb_iteration\n",
    "                        , max_bin = lgb_params['max_bin']   \n",
    "                        , num_leaves = lgb_params['num_leaves']\n",
    "                        , min_child_weight = lgb_params['min_sum_hessian_in_leaf']\n",
    "                        , min_split_gain = lgb_params['min_gain_to_split'] \n",
    "                        , colsample_bytree = lgb_params['feature_fraction']\n",
    "                        , subsample = lgb_params['bagging_fraction']\n",
    "                        , subsample_freq = lgb_params['bagging_freq']\n",
    "                        , seed = 1234\n",
    "                       )\n",
    "\n",
    "print (clf)\n",
    "\n",
    "clf.fit(sparse.hstack([train_x,train_blend_x_lgb_01, train_blend_x_xgb_01]).tocsr(), train_y)\n",
    "\n",
    "print (\"Training finished in %d seconds.\" % (time.time()-start))\n",
    "\n",
    "preds_lgb = clf.predict_proba(sparse.hstack([test_x, test_blend_x_lgb_01, test_blend_x_xgb_01]).tocsr())\n",
    "sub_df_lgb = pd.DataFrame(preds_lgb,columns = [\"low\", \"medium\", \"high\"])\n",
    "sub_df_lgb[\"listing_id\"] = test_data.listing_id.values\n",
    "sub_df_lgb.to_csv(\"../output/sub_lgb_xgb_stacked_lgb.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final submission\n",
    "\n",
    "Let's create our final submission by simply averaging the above two submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = preds_mlp*0.5+preds_lgb*0.5\n",
    "\n",
    "sub_df = pd.DataFrame(preds,columns = [\"low\", \"medium\", \"high\"])\n",
    "sub_df[\"listing_id\"] = test_data.listing_id.values\n",
    "sub_df.to_csv(\"../output/sub_lgb_xgb_stacked_mlp_lgb.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this week we've learnt how to create model ensemble, particularly with model stacking scheme. Keep in mind that the key considerations for building a good ensemble solution are diversity and randomness which can be introduced by:\n",
    "\n",
    "* Deploying heterogeneous algorithms\n",
    "* Using modified version of training data\n",
    "* Randomizing learning algorithms with different parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
